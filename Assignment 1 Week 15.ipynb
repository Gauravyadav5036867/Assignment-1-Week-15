{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cace832-a07d-4181-8fb7-107fab7b3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "Simple linear regression and multiple linear regression are both statistical techniques used to model the relationship between a dependent variable and one or more independent variables. The main difference between them lies in the number of independent variables they consider.\n",
    "\n",
    "1. Simple Linear Regression:\n",
    "\n",
    "Simple linear regression involves only one independent variable to predict the dependent variable.\n",
    "The relationship between the dependent variable and the independent variable is assumed to be linear.\n",
    "The equation for simple linear regression can be represented as:\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+ϵ\n",
    "where \n",
    "�\n",
    "Y is the dependent variable, \n",
    "�\n",
    "X is the independent variable, \n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept, \n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is the slope coefficient, and \n",
    "�\n",
    "ϵ is the error term.\n",
    "Example: Predicting the sales of a product based on advertising expenditure. Here, advertising expenditure (independent variable) is used to predict the sales (dependent variable).\n",
    "\n",
    "2. Multiple Linear Regression:\n",
    "\n",
    "Multiple linear regression involves more than one independent variable to predict the dependent variable.\n",
    "It allows for modeling more complex relationships by considering the combined effect of multiple predictors on the dependent variable.\n",
    "The equation for multiple linear regression can be represented as:\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "where \n",
    "�\n",
    "Y is the dependent variable, \n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,…,X \n",
    "n\n",
    "​\n",
    "  are the independent variables, \n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept, \n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,…,β \n",
    "n\n",
    "​\n",
    "  are the slope coefficients for each independent variable, and \n",
    "�\n",
    "ϵ is the error term.\n",
    "Example: Predicting a person's salary based on their years of experience, level of education, and age. Here, years of experience, level of education, and age are the independent variables used to predict salary (dependent variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e939ed-d745-4de2-a3c1-ee0bc8fccf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "\n",
    "Linear regression relies on several assumptions to be valid. Violation of these assumptions may lead to biased estimates or incorrect conclusions. Here are the key assumptions of linear regression:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable should be linear. This means the change in the dependent variable is proportional to the change in the independent variables.\n",
    "\n",
    "Independence of errors: The errors (residuals) should be independent of each other. This assumption implies that there should be no correlation between consecutive errors.\n",
    "\n",
    "Homoscedasticity: The variance of the errors should be constant across all levels of the independent variables. In other words, the spread of the residuals should be uniform along the range of predicted values.\n",
    "\n",
    "Normality of residuals: The residuals should follow a normal distribution. This assumption means that the errors should be normally distributed with a mean of zero.\n",
    "\n",
    "No perfect multicollinearity: There should be no perfect linear relationship among the independent variables. This means that one independent variable should not be a perfect linear combination of others.\n",
    "\n",
    "No influential outliers: Outliers or influential data points should not excessively affect the results of the regression analysis.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, several diagnostic tools and tests can be used:\n",
    "\n",
    "Residual plots: Plot the residuals (the differences between the observed and predicted values) against the independent variables. This helps to visualize the linearity, homoscedasticity, and presence of outliers.\n",
    "\n",
    "Normality tests: Use statistical tests such as the Shapiro-Wilk test, Kolmogorov-Smirnov test, or visual methods like Q-Q plots to assess the normality of residuals.\n",
    "\n",
    "Multicollinearity diagnostics: Calculate the variance inflation factor (VIF) or examine correlation matrices to detect multicollinearity among independent variables.\n",
    "\n",
    "Influential observations: Use leverage plots, Cook's distance, or DFBETAS to identify influential outliers that might disproportionately influence the regression results.\n",
    "\n",
    "Durbin-Watson test: This test checks for the independence of errors. A value close to 2 suggests no autocorrelation in the residuals.\n",
    "\n",
    "Breusch-Pagan test: This test checks for homoscedasticity. It compares the residual variance for different levels of the independent variables.\n",
    "\n",
    "Jarque-Bera test: This test assesses the normality of residuals by examining skewness and kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c6525-cf76-42e2-9d2b-aa5d8e6027b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "\n",
    "In a linear regression model of the form \n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+ϵ, where \n",
    "�\n",
    "Y is the dependent variable, \n",
    "�\n",
    "X is the independent variable, \n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept, \n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is the slope coefficient, and \n",
    "�\n",
    "ϵ is the error term, the slope and intercept have specific interpretations:\n",
    "\n",
    "Intercept (\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ): The intercept represents the value of the dependent variable \n",
    "�\n",
    "Y when the independent variable \n",
    "�\n",
    "X is zero. It indicates the baseline or starting point of the relationship between the variables.\n",
    "\n",
    "Slope (\n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ): The slope represents the change in the dependent variable \n",
    "�\n",
    "Y for a one-unit change in the independent variable \n",
    "�\n",
    "X. It quantifies the rate of change in \n",
    "�\n",
    "Y with respect to changes in \n",
    "�\n",
    "X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c22ef-bc31-401d-b57e-fa298003c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "\n",
    "Gradient descent is a first-order optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent (negative gradient) with respect to the parameters of the function. It is a fundamental optimization technique widely employed in machine learning for training models.\n",
    "\n",
    "Here's how gradient descent works:\n",
    "\n",
    "Initialization: Start with an initial guess for the parameters (weights) of the model. This can be random or based on some prior knowledge.\n",
    "\n",
    "Compute the gradient: Calculate the gradient (partial derivatives) of the cost function with respect to each parameter. The gradient points in the direction of the greatest rate of increase of the cost function.\n",
    "\n",
    "Update the parameters: Adjust the parameters in the direction opposite to the gradient to minimize the cost function. This adjustment is made using the formula:\n",
    "New parameter\n",
    "=\n",
    "Old parameter\n",
    "−\n",
    "learning rate\n",
    "×\n",
    "gradient\n",
    "New parameter=Old parameter−learning rate×gradient\n",
    "The learning rate determines the size of the steps taken during each iteration. It's a hyperparameter that needs to be carefully chosen because a too small learning rate may result in slow convergence, while a too large learning rate may cause overshooting or divergence.\n",
    "\n",
    "Repeat steps 2 and 3: Iteratively update the parameters until the algorithm converges to a minimum or until a predefined number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66f48e-6030-4539-b194-64ca2c3a8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression that allows for modeling the relationship between a dependent variable and multiple independent variables. In multiple linear regression, the model predicts the value of the dependent variable based on the linear combination of two or more independent variables, each with its own coefficient.\n",
    "\n",
    "The multiple linear regression model can be represented as:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "Y is the dependent variable (the variable being predicted).\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,…,X \n",
    "n\n",
    "​\n",
    "  are the independent variables (features).\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept term (the value of \n",
    "�\n",
    "Y when all independent variables are zero).\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,…,β \n",
    "n\n",
    "​\n",
    "  are the coefficients (slopes) associated with each independent variable, representing the change in \n",
    "�\n",
    "Y for a one-unit change in the corresponding independent variable.\n",
    "�\n",
    "ϵ is the error term, representing the difference between the observed and predicted values of \n",
    "�\n",
    "Y.\n",
    "The multiple linear regression model differs from simple linear regression in that it can accommodate more than one independent variable. This allows for modeling more complex relationships between the dependent variable and multiple predictors. While simple linear regression deals with a single independent variable, multiple linear regression allows for the examination of how multiple factors collectively influence the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83781ed-24ae-4be7-a2a8-7b890b7052f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6\n",
    "\n",
    "Multicollinearity refers to the presence of high correlations among independent variables in a multiple linear regression model. It can cause issues in the estimation of regression coefficients, leading to unreliable and unstable results. Multicollinearity does not affect the predictive power of the model, but it can make the interpretation of individual coefficients difficult and may lead to inflated standard errors.\n",
    "\n",
    "Here's how multicollinearity can manifest and its implications:\n",
    "\n",
    "High correlations: Multicollinearity occurs when two or more independent variables are highly correlated with each other. This means that one independent variable can be approximately predicted from the others.\n",
    "\n",
    "Implications:\n",
    "\n",
    "It becomes challenging to assess the individual effect of each independent variable on the dependent variable because their effects are confounded.\n",
    "Estimates of the regression coefficients become unstable, leading to large standard errors.\n",
    "The precision of the coefficient estimates decreases, making it difficult to identify statistically significant predictors.\n",
    "Detecting and addressing multicollinearity:\n",
    "\n",
    "Correlation matrix: Calculate the correlation matrix between all pairs of independent variables. A correlation coefficient close to +1 or -1 indicates high collinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF quantifies how much the variance of a coefficient is inflated due to multicollinearity. High VIF values (typically above 10) indicate multicollinearity.\n",
    "\n",
    "Eigenvalues: Calculate the eigenvalues of the correlation matrix. Small eigenvalues suggest multicollinearity.\n",
    "\n",
    "Tolerance: Tolerance is the reciprocal of the VIF. A tolerance value close to 1 indicates little multicollinearity.\n",
    "\n",
    "Addressing multicollinearity:\n",
    "\n",
    "Remove redundant variables: If two or more independent variables are highly correlated, consider removing one of them from the model.\n",
    "\n",
    "Combine correlated variables: Instead of using individual variables, create composite variables through techniques like principal component analysis (PCA) or factor analysis.\n",
    "\n",
    "Regularization: Techniques like Ridge regression and Lasso regression penalize large coefficients, reducing the impact of multicollinearity on the coefficient estimates.\n",
    "\n",
    "Collect more data: Increasing the sample size can sometimes mitigate multicollinearity issues by providing more information to estimate the coefficients accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe716554-5795-4c2f-8938-100aacdd8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7\n",
    "\n",
    "\n",
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable \n",
    "�\n",
    "X and the dependent variable \n",
    "�\n",
    "Y is modeled as an nth-degree polynomial function of \n",
    "�\n",
    "X. In polynomial regression, the relationship between the variables is not assumed to be linear, allowing for more flexibility in modeling complex relationships.\n",
    "\n",
    "The polynomial regression model can be represented as:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    " +ϵ\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "Y is the dependent variable.\n",
    "�\n",
    "X is the independent variable.\n",
    "�\n",
    "0\n",
    ",\n",
    "�\n",
    "1\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "0\n",
    "​\n",
    " ,β \n",
    "1\n",
    "​\n",
    " ,…,β \n",
    "n\n",
    "​\n",
    "  are the coefficients.\n",
    "�\n",
    "ϵ is the error term.\n",
    "In contrast to linear regression, where the relationship between \n",
    "�\n",
    "X and \n",
    "�\n",
    "Y is assumed to be linear, polynomial regression allows for curvature in the relationship. By introducing higher-order terms like \n",
    "�\n",
    "2\n",
    ",\n",
    "�\n",
    "3\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "X \n",
    "2\n",
    " ,X \n",
    "3\n",
    " ,…,X \n",
    "n\n",
    " , polynomial regression can capture nonlinear patterns in the data.\n",
    "\n",
    "Key differences between polynomial regression and linear regression:\n",
    "\n",
    "Model complexity: Polynomial regression can capture nonlinear relationships between the independent and dependent variables, whereas linear regression assumes a linear relationship.\n",
    "\n",
    "Degree of the polynomial: In polynomial regression, the degree of the polynomial (n) determines the complexity of the model and the flexibility in capturing curvature in the data. Higher degrees allow for more complex curves but may also lead to overfitting if not properly controlled.\n",
    "\n",
    "Interpretation: Interpretation of coefficients in polynomial regression is more complex compared to linear regression, as each coefficient corresponds to a different power of the independent variable. Higher-order terms can have nonlinear effects on the dependent variable.\n",
    "\n",
    "Extrapolation: Polynomial regression can be prone to overfitting, especially with higher degrees of polynomial, which may lead to unreliable predictions when extrapolating beyond the range of the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a080d44-f3e5-46fc-80d8-25bf678e41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
